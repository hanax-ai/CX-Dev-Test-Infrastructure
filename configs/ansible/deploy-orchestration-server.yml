# configs/ansible/deploy-orchestration-server.yml
---
- name: Deploy Orchestration Server - Phase 4.1
  hosts: hx-orc-server
  gather_facts: yes
  become: yes
  vars:
    model_dir: "/opt/ai_models"
    ollama_service_file: "/etc/systemd/system/ollama.service"
    embedding_models:
      - name: "mxbai-embed-large"
        tag: "335m"
        size: "334M"
      - name: "nomic-embed-text"
        tag: "v1.5"
        size: "137M"
      - name: "all-minilm"
        tag: "33m"
        size: "23M"

  tasks:
    - name: 1. Verify CUDA environment is properly configured
      shell: bash -c "source /etc/profile.d/cuda.sh && nvcc --version"
      register: cuda_verification
      changed_when: false
      failed_when: cuda_verification.rc != 0

    - name: 2. Display CUDA verification
      debug:
        msg: 
          - "CUDA Environment Verified:"
          - "{{ cuda_verification.stdout_lines }}"

    - name: 3. Install Ollama service
      shell: curl -fsSL https://ollama.com/install.sh | sh
      register: ollama_install
      changed_when: "'Ollama is now installed' in ollama_install.stdout or ollama_install.rc == 0"

    - name: 4. Check if Ollama service file exists
      stat:
        path: "{{ ollama_service_file }}"
      register: ollama_service_exists

    - name: 5. Configure Ollama service for GPU and remote access
      blockinfile:
        path: "{{ ollama_service_file }}"
        block: |
          Environment="OLLAMA_HOST=0.0.0.0"
          Environment="OLLAMA_ORIGINS=*"
          Environment="CUDA_VISIBLE_DEVICES=0"
          Environment="PATH=/usr/local/cuda-12.9/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
          Environment="LD_LIBRARY_PATH=/usr/local/cuda-12.9/lib64"
        marker: "# {mark} ANSIBLE MANAGED OLLAMA ENVIRONMENT"
        insertafter: "\\[Service\\]"
        backup: yes
      when: ollama_service_exists.stat.exists
      notify: restart_ollama

    - name: 6. Create AI models directory
      file:
        path: "{{ model_dir }}"
        state: directory
        owner: agent0
        group: agent0
        mode: '0755'

    - name: 6.1. Ensure /opt/ai_models has correct permissions for Ollama
      file:
        path: /opt/ai_models
        state: directory
        owner: ollama
        group: ollama
        recurse: yes
      when: "'ai_processing_tier' in group_names"

    - name: 7. Start and enable Ollama service
      systemd:
        name: ollama
        state: restarted
        enabled: yes
        daemon_reload: yes

    - name: 8. Wait for Ollama service to be ready
      wait_for:
        port: 11434
        host: 127.0.0.1
        delay: 10
        timeout: 60

    - name: 9. Test Ollama service accessibility
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:11434/api/tags"
        method: GET
        timeout: 10
      register: ollama_api_test
      retries: 3
      delay: 5

    - name: 10. Display Ollama service status
      debug:
        msg:
          - "Ollama Service Status: {{ 'RUNNING' if ollama_api_test.status == 200 else 'FAILED' }}"
          - "API Endpoint: http://{{ ansible_default_ipv4.address }}:11434"
          - "Response Code: {{ ollama_api_test.status | default('N/A') }}"

    - name: 11. Pull embedding models
      shell: "ollama pull {{ item.name }}:{{ item.tag }}"
      loop: "{{ embedding_models }}"
      register: model_pulls
      become_user: root
      environment:
        OLLAMA_HOST: "http://localhost:11434"
      retries: 2
      delay: 10

    - name: 12. Verify models are installed
      shell: "ollama list"
      register: installed_models
      changed_when: false

    - name: 13. Display installed models
      debug:
        msg:
          - "Successfully Installed Embedding Models:"
          - "{{ installed_models.stdout_lines }}"

    - name: 14. Test embedding model functionality
      shell: |
        echo '{"model": "mxbai-embed-large:335m", "prompt": "Hello world"}' | \
        curl -s -X POST http://localhost:11434/api/embeddings -d @-
      register: embedding_test
      changed_when: false
      failed_when: false

    - name: 15. Configure firewall for Ollama
      ufw:
        rule: allow
        port: 11434
        from_ip: 192.168.10.0/24
        comment: "Ollama API - Internal network only"
      notify: reload_ufw

    - name: 16. Generate deployment summary
      set_fact:
        orchestration_deployment:
          server: "{{ inventory_hostname }}"
          ip_address: "{{ ansible_default_ipv4.address }}"
          ollama_version: "{{ ollama_install.stdout | regex_search('version [0-9.]+') | default('Unknown') }}"
          service_status: "{{ 'RUNNING' if ollama_api_test.status == 200 else 'FAILED' }}"
          api_endpoint: "http://{{ ansible_default_ipv4.address }}:11434"
          installed_models: "{{ embedding_models | length }}"
          embedding_test: "{{ 'SUCCESS' if 'embedding' in embedding_test.stdout else 'FAILED' }}"
          deployment_time: "{{ ansible_date_time.iso8601 }}"
          cuda_configured: "{{ cuda_verification.rc == 0 }}"

    - name: 17. Display final deployment status
      debug:
        msg:
          - "============================================================="
          - " ORCHESTRATION SERVER DEPLOYMENT COMPLETE"
          - " Server: {{ inventory_hostname }} ({{ ansible_default_ipv4.address }})"
          - "============================================================="
          - "ðŸš€ OLLAMA SERVICE:"
          - "  Status: {{ 'RUNNING' if ollama_api_test.status == 200 else 'FAILED' }}"
          - "  API Endpoint: http://{{ ansible_default_ipv4.address }}:11434"
          - "  GPU Acceleration: {{ 'ENABLED' if cuda_verification.rc == 0 else 'DISABLED' }}"
          - ""
          - "ðŸ¤– EMBEDDING MODELS DEPLOYED:"
          - "{% for model in embedding_models %}"
          - "  âœ… {{ model.name }}:{{ model.tag }} ({{ model.size }})"
          - "{% endfor %}"
          - ""
          - "ðŸ”§ CONFIGURATION:"
          - "  Model Directory: {{ model_dir }}"
          - "  Remote Access: Enabled (0.0.0.0:11434)"
          - "  Firewall: Configured for internal network"
          - "  CUDA Environment: {{ 'CONFIGURED' if cuda_verification.rc == 0 else 'NOT CONFIGURED' }}"
          - ""
          - "ðŸŽ¯ DEPLOYMENT STATUS: {{ 'SUCCESS' if ollama_api_test.status == 200 else 'NEEDS ATTENTION' }}"
          - "============================================================="

    - name: 18. Save deployment report
      copy:
        content: "{{ orchestration_deployment | to_nice_json }}"
        dest: "/tmp/orchestration_deployment_{{ ansible_date_time.epoch }}.json"
      delegate_to: localhost
      become: false

  handlers:
    - name: restart_ollama
      systemd:
        name: ollama
        state: restarted
        daemon_reload: yes

    - name: reload_ufw
      ufw:
        state: reloaded
