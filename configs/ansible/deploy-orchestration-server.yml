# configs/ansible/deploy-orchestration-server.yml
---
- name: Deploy Orchestration Server - Phase 4.1
  hosts: hx-orc-server
  gather_facts: yes
  become: yes
  vars:
    model_dir: "/opt/ai_models"
    ollama_service_file: "/etc/systemd/system/ollama.service"
    # Ollama installer checksum - Update when Ollama releases new installer
    # Last verified: 2025-08-04
    # To update: curl -fsSL https://ollama.com/install.sh | sha256sum
    ollama_installer_checksum: "sha256:9f5f4c4ed21821ba9b847bf3607ae75452283276cd8f52d2f2b38ea9f27af344"
    embedding_models:
      - name: "mxbai-embed-large"
        tag: "335m"
        size: "334M"
      - name: "nomic-embed-text"
        tag: "v1.5"
        size: "137M"
      - name: "all-minilm"
        tag: "33m"
        size: "23M"

  tasks:
    - name: 0. Verify this is the orchestration server
      fail:
        msg: "This playbook should only run on the orchestration server (hx-orc-server)"
      when: inventory_hostname != 'hx-orc-server'

    - name: 1. Verify CUDA environment is properly configured
      shell: bash -c "source /etc/profile.d/cuda.sh && nvcc --version"
      register: cuda_verification
      changed_when: false
      failed_when: cuda_verification.rc != 0

    - name: 2. Display CUDA verification
      debug:
        msg: 
          - "CUDA Environment Verified:"
          - "{{ cuda_verification.stdout_lines }}"

    - name: 3. Download Ollama installer script with integrity verification
      get_url:
        url: https://ollama.com/install.sh
        dest: /tmp/ollama-install.sh
        mode: '0755'
        timeout: 30
        checksum: "{{ ollama_installer_checksum }}"
      register: ollama_script

    - name: 4. Install Ollama from verified script
      command: /tmp/ollama-install.sh
      args:
        creates: /usr/local/bin/ollama
      register: ollama_install
      when: ollama_script is succeeded

    - name: 4.1. Verify Ollama installation
      stat:
        path: /usr/local/bin/ollama
      register: ollama_binary
      failed_when: not ollama_binary.stat.exists

    - name: 4.2. Display installation verification
      debug:
        msg:
          - "Ollama installation verified: /usr/local/bin/ollama exists"
          - "Installer checksum validated: {{ ollama_installer_checksum }}"

    - name: 4.3. Log security verification
      lineinfile:
        path: "/var/log/ollama-deployment-security.log"
        line: "{{ ansible_date_time.iso8601 }} - Ollama installer checksum verified: {{ ollama_installer_checksum }}"
        create: yes
        mode: '0640'

    - name: 5. Check if Ollama service file exists
      stat:
        path: "{{ ollama_service_file }}"
      register: ollama_service_exists

    - name: 6. Configure Ollama service for GPU and remote access
      blockinfile:
        path: "{{ ollama_service_file }}"
        block: |
          Environment="OLLAMA_HOST=0.0.0.0"
          Environment="OLLAMA_ORIGINS=*"
          Environment="CUDA_VISIBLE_DEVICES=0"
          Environment="PATH=/usr/local/cuda-12.9/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
          Environment="LD_LIBRARY_PATH=/usr/local/cuda-12.9/lib64"
        marker: "# {mark} ANSIBLE MANAGED OLLAMA ENVIRONMENT"
        insertafter: "\\[Service\\]"
        backup: yes
      when: ollama_service_exists.stat.exists
      notify: restart_ollama

    - name: 7. Create AI models directory
      file:
        path: "{{ model_dir }}"
        state: directory
        owner: agent0
        group: agent0
        mode: '0755'

    - name: 7.1. Ensure /opt/ai_models has correct permissions for Ollama
      file:
        path: /opt/ai_models
        state: directory
        owner: ollama
        group: ollama
        recurse: yes
      when: "'ai_processing_tier' in group_names"

    - name: 8. Start and enable Ollama service
      systemd:
        name: ollama
        state: restarted
        enabled: yes
        daemon_reload: yes

    - name: 9. Wait for Ollama service to be ready
      wait_for:
        port: 11434
        host: 127.0.0.1
        delay: 10
        timeout: 60

    - name: 10. Test Ollama service accessibility
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:11434/api/tags"
        method: GET
        timeout: 10
      register: ollama_api_test
      retries: 3
      delay: 5

    - name: 11. Display Ollama service status
      debug:
        msg:
          - "Ollama Service Status: {{ 'RUNNING' if ollama_api_test.status == 200 else 'FAILED' }}"
          - "API Endpoint: http://{{ ansible_default_ipv4.address }}:11434"
          - "Response Code: {{ ollama_api_test.status | default('N/A') }}"

    - name: 12. Pull embedding models
      shell: "ollama pull {{ item.name }}:{{ item.tag }}"
      loop: "{{ embedding_models }}"
      register: model_pulls
      become_user: root
      environment:
        OLLAMA_HOST: "http://localhost:11434"
      retries: 2
      delay: 10

    - name: 13. Verify models are installed
      shell: "ollama list"
      register: installed_models
      changed_when: false

    - name: 14. Display installed models
      debug:
        msg:
          - "Successfully Installed Embedding Models:"
          - "{{ installed_models.stdout_lines }}"

    - name: 15. Test embedding model functionality
      shell: |
        echo '{"model": "mxbai-embed-large:335m", "prompt": "Hello world"}' | \
        curl -s -X POST http://localhost:11434/api/embeddings -d @-
      register: embedding_test
      changed_when: false
      failed_when: false

    - name: 16. Configure firewall for Ollama
      ufw:
        rule: allow
        port: 11434
        from_ip: 192.168.10.0/24
        comment: "Ollama API - Internal network only"
      notify: reload_ufw

    - name: 17. Build model summary lines
      set_fact:
        model_summary: []

    - name: 17.1. Build model display list
      set_fact:
        model_summary: "{{ model_summary + ['  âœ… ' + item.name + ':' + item.tag + ' (' + item.size + ')'] }}"
      loop: "{{ embedding_models }}"

    - name: 18. Generate deployment summary
      set_fact:
        orchestration_deployment:
          server: "{{ inventory_hostname }}"
          ip_address: "{{ ansible_default_ipv4.address }}"
          ollama_version: "{{ ollama_install.stdout | regex_search('version [0-9.]+') | default('Unknown') }}"
          service_status: "{{ 'RUNNING' if ollama_api_test.status == 200 else 'FAILED' }}"
          api_endpoint: "http://{{ ansible_default_ipv4.address }}:11434"
          installed_models: "{{ embedding_models | length }}"
          embedding_test: "{{ 'SUCCESS' if 'embedding' in embedding_test.stdout else 'FAILED' }}"
          deployment_time: "{{ ansible_date_time.iso8601 }}"
          cuda_configured: "{{ cuda_verification.rc == 0 }}"

    - name: 19. Display final deployment status
      debug:
        msg:
          - "============================================================="
          - " ORCHESTRATION SERVER DEPLOYMENT COMPLETE"
          - " Server: {{ inventory_hostname }} ({{ ansible_default_ipv4.address }})"
          - "============================================================="
          - "ðŸš€ OLLAMA SERVICE:"
          - "  Status: {{ 'RUNNING' if ollama_api_test.status == 200 else 'FAILED' }}"
          - "  API Endpoint: http://{{ ansible_default_ipv4.address }}:11434"
          - "  GPU Acceleration: {{ 'ENABLED' if cuda_verification.rc == 0 else 'DISABLED' }}"
          - ""
          - "ðŸ¤– EMBEDDING MODELS DEPLOYED:"
          - "{{ model_summary | join('\n') }}"
          - ""
          - "ðŸ”§ CONFIGURATION:"
          - "  Model Directory: {{ model_dir }}"
          - "  Remote Access: Enabled (0.0.0.0:11434)"
          - "  Firewall: Configured for internal network"
          - "  CUDA Environment: {{ 'CONFIGURED' if cuda_verification.rc == 0 else 'NOT CONFIGURED' }}"
          - ""
          - "ðŸŽ¯ DEPLOYMENT STATUS: {{ 'SUCCESS' if ollama_api_test.status == 200 else 'NEEDS ATTENTION' }}"
          - "============================================================="

    - name: 20. Save deployment report
      copy:
        content: "{{ orchestration_deployment | to_nice_json }}"
        dest: "/tmp/orchestration_deployment_{{ ansible_date_time.epoch }}.json"
      delegate_to: localhost
      become: false

    - name: 21. Clean up temporary files
      file:
        path: /tmp/ollama-install.sh
        state: absent

  handlers:
    - name: restart_ollama
      systemd:
        name: ollama
        state: restarted
        daemon_reload: yes

    - name: reload_ufw
      ufw:
        state: reloaded
