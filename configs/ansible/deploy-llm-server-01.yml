# configs/ansible/deploy-llm-server-01.yml
---
- name: Deploy LLM Server 01 - Phase 4.2 (Chat Models)
  hosts: hx-llm-server-01
  gather_facts: yes
  become: yes
  vars:
    model_dir: "/opt/ai_models"
    ollama_service_file: "/etc/systemd/system/ollama.service"
    miniconda_path: "/home/agent0/miniconda3"
    llm_env_name: "llm_env"
    chat_models:
      - name: "llama3"
        tag: "8b"  # Base for chat
      - name: "nous-hermes2"
        tag: "latest"
      - name: "llama3.2"
        tag: "3b"  # 3B Instruct model via Ollama

  tasks:
    - name: 1. Verify CUDA environment
      shell: bash -c 'source /etc/profile.d/cuda.sh && nvcc --version'
      register: cuda_verification
      changed_when: false
      failed_when: cuda_verification.rc != 0

    - name: 2. Display CUDA verification
      debug:
        msg: 
          - "CUDA Environment Verified:"
          - "{{ cuda_verification.stdout_lines }}"

    - name: 3. Install Ollama service
      shell: curl -fsSL https://ollama.com/install.sh | sh
      register: ollama_install
      changed_when: "'Ollama is now installed' in ollama_install.stdout or ollama_install.rc == 0"

    - name: 4. Check Ollama service file
      stat:
        path: "{{ ollama_service_file }}"
      register: ollama_service_exists

    - name: 5. Configure Ollama for GPU and remote access
      blockinfile:
        path: "{{ ollama_service_file }}"
        block: |
          Environment="OLLAMA_HOST=0.0.0.0"
          Environment="OLLAMA_ORIGINS=*"
          Environment="OLLAMA_MODELS={{ model_dir }}"
          Environment="CUDA_VISIBLE_DEVICES=0,1"
          Environment="PATH=/usr/local/cuda-12.9/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
          Environment="LD_LIBRARY_PATH=/usr/local/cuda-12.9/lib64"
        marker: "# {mark} ANSIBLE MANAGED OLLAMA ENVIRONMENT"
        insertafter: "\\[Service\\]"
        backup: yes
      when: ollama_service_exists.stat.exists
      notify: restart_ollama

    - name: 6. Create AI models directory (on NVMe/SSD/HDD)
      file:
        path: "{{ model_dir }}"
        state: directory
        owner: agent0
        group: agent0
        mode: '0755'

    - name: 7. Start and enable Ollama
      systemd:
        name: ollama
        state: started
        enabled: yes
        daemon_reload: yes

    - name: 8. Wait for Ollama ready
      wait_for:
        port: 11434
        host: 0.0.0.0
        delay: 5
        timeout: 30

    - name: 9. Test Ollama accessibility
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:11434/api/tags"
        method: GET
        timeout: 10
      register: ollama_api_test
      retries: 3
      delay: 5

    - name: 10. Pull all chat models with Ollama
      shell: "ollama pull {{ item.name }}:{{ item.tag }}"
      loop: "{{ chat_models }}"
      register: model_pulls
      become_user: root
      environment:
        OLLAMA_HOST: "http://localhost:11434"
      retries: 2
      delay: 10

    - name: 11. Verify all models
      shell: "ollama list"
      register: installed_models
      changed_when: false

    - name: 12. Test chat model
      shell: |
        echo '{"model": "llama3:8b", "prompt": "Hello as chat model"}' | \
        curl -s -X POST http://localhost:11434/api/chat -d @-
      register: chat_test
      changed_when: false
      failed_when: false

    - name: 13. Configure firewall
      ufw:
        rule: allow
        port: 11434
        from_ip: 192.168.10.0/24
        comment: "Ollama API - Internal only"
      notify: reload_ufw

    - name: 14. Generate deployment summary
      set_fact:
        llm01_deployment:
          server: "{{ inventory_hostname }}"
          ip_address: "{{ ansible_default_ipv4.address }}"
          ollama_version: "{{ ollama_install.stdout | regex_search('version [0-9.]+') | default('Unknown') }}"
          service_status: "{{ 'RUNNING' if ollama_api_test.status == 200 else 'FAILED' }}"
          api_endpoint: "http://{{ ansible_default_ipv4.address }}:11434"
          installed_models: "{{ chat_models | length }}"
          chat_test: "{{ 'SUCCESS' if 'response' in chat_test.stdout else 'FAILED' }}"
          deployment_time: "{{ ansible_date_time.iso8601 }}"
          cuda_configured: "{{ cuda_verification.rc == 0 }}"

    - name: 15. Display status
      debug:
        msg:
          - "============================================================="
          - " LLM SERVER 01 DEPLOYMENT COMPLETE"
          - " Server: {{ inventory_hostname }} ({{ ansible_default_ipv4.address }})"
          - "============================================================="
          - "ðŸš€ OLLAMA SERVICE:"
          - "  Status: {{ 'RUNNING' if ollama_api_test.status == 200 else 'FAILED' }}"
          - "  API Endpoint: http://{{ ansible_default_ipv4.address }}:11434"
          - "  GPU Acceleration: {{ 'ENABLED' if cuda_verification.rc == 0 else 'DISABLED' }}"
          - ""
          - "ðŸ¤– CHAT MODELS DEPLOYED:"
          - "{% for model in chat_models %}"
          - "  âœ… {{ model.name }}:{{ model.tag }}"
          - "{% endfor %}"
          - ""
          - "ðŸ”§ CONFIGURATION:"
          - "  Model Directory: {{ model_dir }} (NVMe/SSD/HDD)"
          - "  Python Environment: {{ llm_env_name }}"
          - "  Remote Access: Enabled"
          - "  Firewall: Configured"
          - "  CUDA: {{ 'CONFIGURED' if cuda_verification.rc == 0 else 'NOT' }}"
          - ""
          - "ðŸŽ¯ STATUS: {{ 'SUCCESS' if ollama_api_test.status == 200 else 'ATTENTION' }}"
          - "============================================================="

    - name: 16. Save report
      copy:
        content: "{{ llm01_deployment | to_nice_json }}"
        dest: "/tmp/llm01_deployment_{{ ansible_date_time.epoch }}.json"
      delegate_to: localhost
      become: false

  handlers:
    - name: restart_ollama
      systemd:
        name: ollama
        state: restarted
        daemon_reload: yes

    - name: reload_ufw
      ufw:
        state: reloaded
