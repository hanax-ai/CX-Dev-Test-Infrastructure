---
- name: Deploy LLM Server 02 - Simplified Approach
  hosts: hx-llm-server-02
  gather_facts: yes
  become: yes
  vars:
    model_dir: "/opt/ai_models"
    ollama_service_user: "ollama"
    ollama_port: 11434

  pre_tasks:
    - name: Verify this is the correct server
      fail:
        msg: "This playbook should only run on hx-llm-server-02"
      when: inventory_hostname != 'hx-llm-server-02'

    - name: Check available disk space
      shell: df -h /opt
      register: disk_space
      changed_when: false

    - name: Display disk space
      debug:
        msg: "Available disk space: {{ disk_space.stdout_lines[1] }}"

  tasks:
    - name: 1. Check current GPU status
      shell: nvidia-smi --list-gpus
      register: gpu_list
      changed_when: false

    - name: 2. Display GPU status
      debug:
        msg:
          - "GPU Detection:"
          - "{{ gpu_list.stdout_lines }}"

    - name: 3. Check CUDA driver
      shell: cat /proc/driver/nvidia/version
      register: nvidia_driver
      changed_when: false
      failed_when: false

    - name: 4. Display CUDA driver info
      debug:
        msg:
          - "NVIDIA Driver:"
          - "{{ nvidia_driver.stdout_lines if nvidia_driver.rc == 0 else 'Driver not found' }}"

    - name: 5. Check if Ollama is already installed
      shell: which ollama
      register: ollama_exists
      changed_when: false
      failed_when: false

    - name: 6. Ensure ollama system user exists
      user:
        name: ollama
        system: yes
        shell: /usr/sbin/nologin
        home: /var/lib/ollama
        create_home: yes
        state: present

    - name: 7. Stop existing Ollama service if running
      systemd:
        name: ollama
        state: stopped
      failed_when: false
      when: ollama_exists.rc == 0

    - name: 8. Download Ollama installer
      get_url:
        url: https://ollama.com/install.sh
        dest: /tmp/ollama_install.sh
        mode: '0755'
        force: yes
        timeout: 30

    - name: 9. Run Ollama installer (idempotent)
      command: /tmp/ollama_install.sh
      args:
        creates: /usr/local/bin/ollama
      register: ollama_install

    - name: 10. Create secure service configuration
      copy:
        content: |
          [Unit]
          Description=Ollama Service
          After=network-online.target
          Wants=network-online.target
          
          [Service]
          Type=simple
          ExecStart=/usr/local/bin/ollama serve
          User={{ ollama_service_user }}
          Group={{ ollama_service_user }}
          Restart=always
          RestartSec=3
          Environment="OLLAMA_HOST=0.0.0.0:{{ ollama_port }}"
          Environment="OLLAMA_ORIGINS=*"
          Environment="OLLAMA_MODELS=/var/lib/ollama/models"
          # Security hardening
          NoNewPrivileges=true
          PrivateTmp=true
          ProtectSystem=strict
          ProtectHome=true
          ReadWritePaths=/var/lib/ollama
          
          [Install]
          WantedBy=multi-user.target
        dest: /etc/systemd/system/ollama.service
        backup: yes
        mode: '0644'

    - name: 11. Create AI models directory
      file:
        path: "{{ model_dir }}"
        state: directory
        owner: agent0
        group: agent0
        mode: '0755'

    - name: 12. Reload systemd and start Ollama
      systemd:
        name: ollama
        state: started
        enabled: yes
        daemon_reload: yes

    - name: 13. Wait for service to be ready
      wait_for:
        port: "{{ ollama_port }}"
        host: "{{ ansible_default_ipv4.address | default('127.0.0.1') }}"
        delay: 5
        timeout: 60
        msg: "Ollama service failed to start within timeout period"

    - name: 14. Verify service is responding
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:{{ ollama_port }}/api/version"
        method: GET
        timeout: 10
      register: version_check
      retries: 3
      delay: 5
      until: version_check.status == 200

    - name: 15. Test basic API connectivity
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:{{ ollama_port }}/api/tags"
        method: GET
        timeout: 10
      register: api_test

    - name: 16. Display API test result
      debug:
        msg:
          - "API Test Result:"
          - "Status: {{ api_test.status }}"
          - "Service is {{ 'WORKING' if api_test.status == 200 else 'FAILED' }}"

    - name: 17. List current models
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:{{ ollama_port }}/api/tags"
        method: GET
      register: current_models

    - name: 18. Display current models
      debug:
        msg:
          - "Current Models:"
          - "{{ current_models.json.models if current_models.json is defined and current_models.json.models is defined else 'No models found' }}"

    - name: 19. Check service status
      shell: systemctl status ollama --no-pager
      register: service_status
      changed_when: false

    - name: 20. Display service status
      debug:
        msg:
          - "Service Status:"
          - "{{ service_status.stdout_lines }}"

    - name: 21. Clean up temporary files
      file:
        path: /tmp/ollama_install.sh
        state: absent

  post_tasks:
    - name: Final validation - Ensure service is active
      systemd:
        name: ollama
        state: started
      register: final_service_check

    - name: Final validation - Test API endpoint
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:{{ ollama_port }}/api/version"
        method: GET
        timeout: 5
      register: final_api_check
      failed_when: final_api_check.status != 200

    - name: Log deployment success
      lineinfile:
        path: /var/log/llm-server-02-deployment.log
        line: "LLM Server 02 deployment completed successfully at {{ ansible_date_time.iso8601 }}"
        create: yes
      when: final_api_check.status == 200

    - name: 22. Display next steps
      debug:
        msg:
          - "============================================================="
          - " BASIC OLLAMA SERVICE DEPLOYMENT COMPLETE"
          - "============================================================="
          - "ðŸš€ SERVICE STATUS:"
          - "  API Endpoint: http://{{ ansible_default_ipv4.address }}:{{ ollama_port }}"
          - "  Status: {{ 'RUNNING' if api_test.status == 200 else 'FAILED' }}"
          - ""
          - "ðŸ“‹ NEXT STEPS TO COMPLETE MANUALLY:"
          - "  1. SSH to server: ssh agent0@{{ ansible_default_ipv4.address }}"
          - "  2. Remove unwanted model: ollama rm llama3.2:3b"
          - "  3. Test existing models:"
          - "     ollama run qwen3:8b"
          - "     ollama run mistral:7b"
          - "     ollama run nous-hermes2:latest"
          - "     ollama run llama4:16x17b"
          - "  4. Check GPU usage: nvidia-smi"
          - "  5. Check service logs: journalctl -u ollama -f"
          - ""
          - "ðŸ”§ IF ISSUES PERSIST:"
          - "  â€¢ Check CUDA: nvidia-smi"
          - "  â€¢ Check driver: cat /proc/driver/nvidia/version"
          - "  â€¢ Restart service: sudo systemctl restart ollama"
          - "============================================================="

  handlers:
    - name: reload systemd
      systemd:
        daemon_reload: yes

    - name: restart ollama
      systemd:
        name: ollama
        state: restarted

    - name: start ollama
      systemd:
        name: ollama
        state: started
        enabled: yes
