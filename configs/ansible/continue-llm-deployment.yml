# Continue LLM Server 01 deployment from model pulling
---
- name: Continue LLM Server 01 Deployment - Model Installation
  hosts: hx-llm-server-01
  gather_facts: yes
  become: yes
  vars:
    miniconda_path: "/home/agent0/miniconda3"
    ai_env_name: "ai_env"
    chat_models:
      - name: "llama3"
        tag: "8b"
      - name: "nous-hermes2"
        tag: "latest"
    additional_model_id: "Llama3.2-3B-Instruct"
    additional_model_url: "https://llama3-2-lightweight.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZng5YzkxamYyOTFta283OWdrMDZuNGc5IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvbGxhbWEzLTItbGlnaHR3ZWlnaHQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDE1MTk4NX19fV19&Signature=UkwG9f0v2DHpI3RDfK-kGQuSpIhhpAztsNqnI5uJKTFIciPoPiV4LHjkEHqXtv4Y14tSAvY%7Ex3LCKyr8V01mxCa1IxwNzPn7UouCeLdWq7LZV%7ECWzyRYI4iIRZ9QeFIs8thqm98PpmMWRpcsxW2ZopeQJNHIMNaCVoGkKPdBbkkGzIhdbSMFfWknaiB0muubL9zkvYt0DYrBmSKE5%7EBeD85olIXQkT6wtozwEHhoDr8Kv9Yvvvl38JjeyBeqoTcvVBRwrY2d6u1Ad4u3zvytf7f0r9gVR-GeMHjB3OLmjrF-l3U47Lek-hW2nAr-T8tGhbLBMexCO5SQLqazUfTX2g__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=785333870720711"

  tasks:
    - name: 1. Test Ollama accessibility
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:11434/api/tags"
        method: GET
        timeout: 10
      register: ollama_api_test
      retries: 3
      delay: 5

    - name: 2. Pull existing chat models with Ollama
      shell: "ollama pull {{ item.name }}:{{ item.tag }}"
      loop: "{{ chat_models }}"
      register: model_pulls
      become_user: root
      environment:
        OLLAMA_HOST: "http://localhost:11434"
      retries: 2
      delay: 10

    - name: 3. Activate ai_env for llama-stack
      shell: . {{ miniconda_path }}/bin/activate {{ ai_env_name }} && pip install llama-stack
      args:
        executable: /bin/bash
      become_user: agent0

    - name: 4. Download additional chat model with Llama CLI (automate prompt)
      expect:
        command: . {{ miniconda_path }}/bin/activate {{ ai_env_name }} && llama model download --source meta --model-id {{ additional_model_id }}
        responses:
          "Enter the custom URL:": "{{ additional_model_url }}"
        timeout: 300
      become_user: agent0
      register: additional_model_download

    - name: 5. Verify all models
      shell: "ollama list"
      register: installed_models
      changed_when: false

    - name: 6. Test chat model
      shell: |
        echo '{"model": "llama3:8b", "prompt": "Hello as chat model"}' | \
        curl -s -X POST http://localhost:11434/api/chat -d @-
      register: chat_test
      changed_when: false
      failed_when: false

    - name: 7. Configure firewall
      ufw:
        rule: allow
        port: 11434
        from_ip: 192.168.10.0/24
        comment: "Ollama API - Internal only"

    - name: 8. Generate deployment summary
      set_fact:
        llm01_deployment:
          server: "{{ inventory_hostname }}"
          ip_address: "{{ ansible_default_ipv4.address }}"
          service_status: "{{ 'RUNNING' if ollama_api_test.status == 200 else 'FAILED' }}"
          api_endpoint: "http://{{ ansible_default_ipv4.address }}:11434"
          installed_models: "{{ chat_models | length + 1 }}"
          chat_test: "{{ 'SUCCESS' if 'response' in chat_test.stdout else 'FAILED' }}"
          deployment_time: "{{ ansible_date_time.iso8601 }}"

    - name: 9. Display final status
      debug:
        msg:
          - "============================================================="
          - " LLM SERVER 01 DEPLOYMENT COMPLETE"
          - " Server: {{ inventory_hostname }} ({{ ansible_default_ipv4.address }})"
          - "============================================================="
          - "ðŸš€ OLLAMA SERVICE:"
          - "  Status: {{ 'RUNNING' if ollama_api_test.status == 200 else 'FAILED' }}"
          - "  API Endpoint: http://{{ ansible_default_ipv4.address }}:11434"
          - "  GPU Acceleration: ENABLED"
          - ""
          - "ðŸ¤– CHAT MODELS DEPLOYED:"
          - "{% for model in chat_models %}"
          - "  âœ… {{ model.name }}:{{ model.tag }}"
          - "{% endfor %}"
          - "  âœ… Llama-3.2-3B-Instruct-QLORA_INT4_EO8 (additional)"
          - ""
          - "ðŸŽ¯ STATUS: {{ 'SUCCESS' if ollama_api_test.status == 200 else 'ATTENTION' }}"
          - "============================================================="
