---
- name: Deploy LLM Server 02 - V3 Final
  hosts: hx-llm-server-02
  gather_facts: yes
  become: yes

  vars:
    # --- General Configuration ---
    model_dir: "/opt/ai_models"
    miniconda_path: "/home/agent0/miniconda3"
    llm_env_name: "llm_env"

    # --- Standard Models from Ollama Registry ---
    ollama_models:
      - name: qwen3:8b
      - name: mistral:7b
      - name: nous-hermes2:latest
      - name: llama4:16x17b

    # --- Custom Meta Model Configuration ---
    meta_model:
      model_id: "Llama-4-Maverick-17B-128E-Instruct-FP8"
      ollama_name: "llama4-maverick-instruct"
    
    # Pre-signed URL for the Meta Model
    meta_presigned_url: "https://llama4.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiOHNmMzJxMm5idGt5YWdvajRibG51bXo5IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvbGxhbWE0LmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NTQyNDEwMDd9fX1dfQ__&Signature=s1I-VUpzB38n9YChsj65-wl%7ESGquwfo9cO49NjqE-fsBnmJiO7QHY7QCILBImxaqYuOplmxo5v%7EaM%7ESnphJUiKoocO43pXV09j3YLJjSp%7EsFbrm7M%7EneE68RqZWxw-lh8IJzM9515q2uhofeQ2hDbtB1wb4ITw6mAx%7Eopo0qEOKzfAnnqlCCqOjAxVSMiERNKhh9JEeu9vh2THD5nTBBljnJA%7EGNgDOHMZDDgknQRq6MS2ChOjLOb9GenCH7YvDVHOA107fgu5OMHiL-M1i1RvBKkQlYbHlF6HVg%7E0qctbgllxy8fCee%7EU2qJh4cjuFPtw-Z2Ci-y0SKB0-YUxyGSA__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1285821186262999"

  pre_tasks:
    - name: 1. Ensure GRUB is patched for GPU stability (pci=realloc=off)
      lineinfile:
        path: /etc/default/grub
        regexp: '^GRUB_CMDLINE_LINUX_DEFAULT='
        line: 'GRUB_CMDLINE_LINUX_DEFAULT="quiet splash pci=realloc=off"'
      notify: update_grub_and_reboot

  tasks:
    # === SECTION 1: SYSTEM & GPU VERIFICATION ===
    - name: 2. Verify dual GPU hardware via lspci
      shell: "lspci | grep -i nvidia | wc -l"
      register: gpu_count
      changed_when: false

    - name: 3. Reload NVIDIA kernel modules for stability
      shell: |
        modprobe -r nvidia_drm nvidia_modeset nvidia_uvm nvidia || true
        modprobe nvidia
      args:
        warn: false
      changed_when: false

    - name: 4. Enable NVIDIA persistence mode for performance
      shell: "nvidia-smi -pm 1"
      changed_when: false

    # === SECTION 2: OLLAMA SETUP & CONFIGURATION ===
    - name: 5. Install Ollama service
      shell: "curl -fsSL https://ollama.com/install.sh | sh"
      args:
        creates: /usr/local/bin/ollama

    - name: 6. Create AI models directory
      file:
        path: "{{ model_dir }}"
        state: directory
        owner: agent0
        group: agent0
        mode: '0755'

    - name: 7. Configure Ollama systemd service for Dual GPU & Remote Access
      blockinfile:
        path: /etc/systemd/system/ollama.service
        block: |
          Environment="OLLAMA_HOST=0.0.0.0"
          Environment="OLLAMA_ORIGINS=*"
          Environment="OLLAMA_MODELS={{ model_dir }}"
          Environment="CUDA_VISIBLE_DEVICES=0,1"
          Environment="OLLAMA_NUM_PARALLEL=4"
          Environment="OLLAMA_MAX_LOADED_MODELS=6"
          Environment="OLLAMA_KEEP_ALIVE=10m"
          Environment="PATH=/usr/local/cuda-12.9/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
          Environment="LD_LIBRARY_PATH=/usr/local/cuda-12.9/lib64:/usr/lib/x86_64-linux-gnu"
        marker: "# {mark} ANSIBLE MANAGED OLLAMA ENV"
        insertafter: "\\[Service\\]"
        backup: yes
      notify: restart_ollama

    - name: 8. Start and enable Ollama service
      systemd:
        name: ollama
        state: started
        enabled: yes
        daemon_reload: yes

    - name: 9. Wait for Ollama API to be ready
      uri:
        url: http://localhost:11434
        status_code: 200
      retries: 5
      delay: 5

    # === SECTION 3: STANDARD MODEL DEPLOYMENT ===
    - name: 10. Pull standard models from Ollama Hub (idempotent)
      command: "ollama pull {{ item.name }}"
      loop: "{{ ollama_models }}"
      args:
        creates: "{{ model_dir }}/manifests/registry.ollama.ai/library/{{ item.name.split(':')[0] }}/latest"
      become_user: agent0

    # === SECTION 4: CUSTOM META MODEL DEPLOYMENT ===
    - name: 11. Ensure llama-stack CLI is installed in llm_env
      pip:
        name: llama-stack
        executable: "{{ miniconda_path }}/envs/{{ llm_env_name }}/bin/pip"
      become_user: agent0

    - name: 12. Download the Meta model using the pre-signed URL
      expect:
        command: "{{ miniconda_path }}/envs/{{ llm_env_name }}/bin/llama model download --source meta --model-id {{ meta_model.model_id }}"
        responses:
          'custom URL:': "{{ meta_presigned_url }}"
        timeout: 3600 # 60 minutes
      args:
        creates: "/home/agent0/.llama/checkpoints/{{ meta_model.model_id }}/params.json"
      become_user: agent0

    - name: 13. Create Modelfile to register Meta model with Ollama
      copy:
        content: "FROM /home/agent0/.llama/checkpoints/{{ meta_model.model_id }}"
        dest: "/tmp/{{ meta_model.ollama_name }}_modelfile"
      when: not "meta_model.ollama_name" in (ollama_list.stdout | default(''))

    - name: 14. Register the Meta model with Ollama
      shell: "ollama create {{ meta_model.ollama_name }} -f /tmp/{{ meta_model.ollama_name }}_modelfile"
      register: model_registration
      become_user: agent0
      when: not "meta_model.ollama_name" in (ollama_list.stdout | default(''))

    # === SECTION 5: FINAL VERIFICATION & REPORTING ===
    - name: 15. Get final list of all registered models
      shell: "ollama list"
      register: ollama_list
      changed_when: false

    - name: 16. Generate Final Deployment Report
      debug:
        msg:
          - "============================================================="
          - " LLM SERVER 02 DEPLOYMENT COMPLETE - V3"
          - " Server: {{ inventory_hostname }} ({{ ansible_default_ipv4.address }})"
          - "============================================================="
          - "ðŸš€ OLLAMA SERVICE:"
          - "  Status: RUNNING"
          - "  API Endpoint: http://{{ ansible_default_ipv4.address }}:11434"
          - "  DUAL GPU Setup: {{ gpu_count.stdout }} GPUs detected"
          - ""
          - "ðŸ¤– MODELS DEPLOYED:"
          - "{{ ollama_list.stdout_lines | join('\\n  - ') }}"
          - ""
          - "  Custom Meta Model ({{ meta_model.model_id }}): {{ 'REGISTERED as ' + meta_model.ollama_name if model_registration.changed else 'ALREADY PRESENT' }}"
          - ""
          - "ðŸŽ¯ OVERALL STATUS: PRODUCTION READY"
          - "============================================================="

  handlers:
    - name: update_grub_and_reboot
      block:
        - name: Update GRUB configuration
          command: update-grub
        - name: Reboot system to apply kernel parameters
          reboot:
            msg: "Rebooting to apply GRUB patch for GPU stability"
            reboot_timeout: 360

    - name: restart_ollama
      systemd:
        name: ollama
        state: restarted
        daemon_reload: yes