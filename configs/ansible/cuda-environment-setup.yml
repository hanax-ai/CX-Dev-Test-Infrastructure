# configs/ansible/cuda-environment-setup.yml
---
- name: Configure CUDA Environment for AI Processing Tier
  hosts: ai_servers
  gather_facts: yes
  become: yes

  vars:
    cuda_version: "12.9"
    cuda_home: "/usr/local/cuda-{{ cuda_version }}"
    cuda_profile_file: "/etc/profile.d/cuda.sh"

  tasks:
    - name: 1. Verify CUDA installation directory exists
      stat:
        path: "{{ cuda_home }}"
      register: cuda_dir_check
      failed_when: not cuda_dir_check.stat.exists

    - name: 2. Create CUDA environment profile script
      copy:
        content: |
          # CUDA Environment Configuration for Citadel Alpha AI Processing Tier
          # Generated by Ansible on {{ ansible_date_time.iso8601 }}
          
          export CUDA_HOME={{ cuda_home }}
          export CUDA_ROOT={{ cuda_home }}
          export PATH={{ cuda_home }}/bin:$PATH
          export LD_LIBRARY_PATH={{ cuda_home }}/lib64:$LD_LIBRARY_PATH
          export CUDA_CACHE_PATH=/tmp/cuda-cache
          
          # Additional CUDA environment variables
          export CUDA_VISIBLE_DEVICES=all
          export NVIDIA_VISIBLE_DEVICES=all
        dest: "{{ cuda_profile_file }}"
        mode: '0644'
        owner: root
        group: root
      register: cuda_profile_created

    - name: 3. Create CUDA cache directory
      file:
        path: /tmp/cuda-cache
        state: directory
        mode: '0755'
        owner: agent0
        group: agent0

    - name: 4. Add CUDA environment to agent0 bashrc
      blockinfile:
        path: /home/agent0/.bashrc
        block: |
          # CUDA Environment Configuration
          source {{ cuda_profile_file }}
        marker: "# {mark} ANSIBLE MANAGED CUDA BLOCK"
        create: yes
        owner: agent0
        group: agent0

    - name: 5. Source CUDA environment and verify nvcc access
      shell: |
        source {{ cuda_profile_file }}
        nvcc --version 2>/dev/null || echo "CUDA configuration failed"
      register: cuda_verification
      changed_when: false

    - name: 6. Test GPU accessibility with CUDA
      shell: |
        source {{ cuda_profile_file }}
        nvidia-smi -L | wc -l
      register: gpu_count_check
      changed_when: false

    - name: 7. Display CUDA configuration results
      debug:
        msg:
          - "============================================================="
          - " CUDA Environment Configuration Results: {{ inventory_hostname }}"
          - "============================================================="
          - "CUDA Home: {{ cuda_home }}"
          - "Profile Created: {{ 'Yes' if cuda_profile_created.changed else 'Already existed' }}"
          - "NVCC Test: {{ cuda_verification.stdout if cuda_verification.stdout else 'Failed' }}"
          - "GPU Count: {{ gpu_count_check.stdout if gpu_count_check.stdout else 'Unknown' }}"
          - "Configuration Status: {{ 'SUCCESS' if 'release' in cuda_verification.stdout else 'NEEDS ATTENTION' }}"
          - "-------------------------------------------------------------"

    - name: 8. Create verification directory
      file:
        path: /opt/citadel
        state: directory
        mode: '0755'
        owner: agent0
        group: agent0

    - name: 8a. Create CUDA verification script for future use
      copy:
        content: |
          #!/bin/bash
          # CUDA Environment Verification Script
          # Generated by Ansible for Citadel Alpha AI Processing Tier
          
          echo "=== CUDA Environment Verification ==="
          echo "CUDA Home: $CUDA_HOME"
          echo "CUDA in PATH: $(which nvcc 2>/dev/null || echo 'Not found')"
          
          echo ""
          echo "=== NVCC Version ==="
          nvcc --version 2>/dev/null || echo "NVCC not accessible"
          
          echo ""
          echo "=== GPU Status ==="
          nvidia-smi -L
          
          echo ""
          echo "=== CUDA Libraries ==="
          echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
          ls -la $CUDA_HOME/lib64/libcudart.so* 2>/dev/null || echo "CUDA runtime libraries not found"
          
          echo ""
          echo "=== Test GPU-CUDA Integration ==="
          python3 -c "
          try:
              import subprocess
              result = subprocess.run(['nvidia-smi', '--query-gpu=name,driver_version,cuda_version', '--format=csv,noheader'], 
                                    capture_output=True, text=True)
              print('GPU-CUDA Status:', result.stdout.strip() if result.returncode == 0 else 'Failed')
          except Exception as e:
              print('Python GPU test failed:', e)
          "
        dest: /opt/citadel/verify-cuda.sh
        mode: '0755'
        owner: agent0
        group: agent0

    - name: 9. Run final CUDA verification
      shell: /opt/citadel/verify-cuda.sh
      register: final_verification
      become_user: agent0
      environment:
        CUDA_HOME: "{{ cuda_home }}"
        PATH: "{{ cuda_home }}/bin:{{ ansible_env.PATH }}"
        LD_LIBRARY_PATH: "{{ cuda_home }}/lib64:{{ ansible_env.LD_LIBRARY_PATH | default('') }}"

    - name: 10. Display final verification results
      debug:
        var: final_verification.stdout_lines
