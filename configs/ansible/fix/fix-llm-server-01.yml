# configs/ansible/fix-llm-server-01.yml
---
- name: Fix LLM Server 01 - Corrective Actions
  hosts: hx-llm-server-01
  gather_facts: yes
  become: yes
  vars:
    model_dir: "/opt/ai_models"
    ollama_service_file: "/etc/systemd/system/ollama.service"
    miniconda_path: "/home/agent0/miniconda3"
    ai_env_name: "ai_env"
    # NOTE: Using ollama pull for standard models instead of presigned URLs for idempotency
    additional_models:
      - "llama3.2:3b"  # Use standard Ollama registry instead of presigned URL

  tasks:
    - name: 1. Create systemd override directory for Ollama
      file:
        path: "/etc/systemd/system/ollama.service.d"
        state: directory
        mode: '0755'

    - name: 2. Configure Ollama service environment via drop-in
      copy:
        dest: "/etc/systemd/system/ollama.service.d/override.conf"
        content: |
          [Service]
          Environment="OLLAMA_HOST=0.0.0.0"
          Environment="OLLAMA_ORIGINS=*"
          Environment="OLLAMA_MODELS={{ model_dir }}"
          Environment="CUDA_VISIBLE_DEVICES=0,1"
          Environment="PATH=/usr/local/cuda-12.9/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
          Environment="LD_LIBRARY_PATH=/usr/local/cuda-12.9/lib64"
        mode: '0644'
      notify: restart_ollama_fix

    - name: 3. Stop Ollama service
      systemd:
        name: ollama
        state: stopped

    - name: 4. Ensure proper ownership of model directory
      file:
        path: "{{ model_dir }}"
        state: directory
        owner: ollama
        group: ollama
        mode: '0755'
        recurse: yes

    - name: 5. Reload systemd and restart Ollama
      systemd:
        name: ollama
        state: started
        enabled: yes
        daemon_reload: yes

    - name: 6. Wait for Ollama to be ready
      wait_for:
        port: 11434
        host: 0.0.0.0
        delay: 10
        timeout: 60

    - name: 7. Test Ollama API accessibility
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:11434/api/tags"
        method: GET
        timeout: 10
      register: ollama_api_test
      retries: 3
      delay: 5

    - name: 8. Re-pull existing models to new location with retry logic
      shell: "ollama pull {{ item }}"
      loop:
        - "llama3:8b"
        - "nous-hermes2:latest"
      register: model_repulls
      environment:
        OLLAMA_HOST: "http://localhost:11434"
      retries: 2
      delay: 10
      until: model_repulls.rc == 0

    - name: 9. Pull additional models (using Ollama registry)
      shell: "ollama pull {{ item }}"
      loop: "{{ additional_models }}"
      register: additional_model_pulls
      environment:
        OLLAMA_HOST: "http://localhost:11434"
      retries: 2
      delay: 10
      until: additional_model_pulls.rc == 0
      failed_when: false  # Make this optional since model may not be available

    - name: 10. Verify models in new location
      shell: "ls -la {{ model_dir }}/"
      register: model_directory_contents
      changed_when: false

    - name: 11. Verify all models via API
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:11434/api/tags"
        method: GET
        timeout: 10
      register: final_model_check

    - name: 12. Display corrective action results
      debug:
        msg:
          - "============================================================="
          - " LLM SERVER 01 CORRECTIVE ACTIONS COMPLETE"
          - "============================================================="
          - "üîß CONFIGURATION FIXES:"
          - "  OLLAMA_MODELS: {{ model_dir }}"
          - "  Service Status: {{ 'RUNNING' if ollama_api_test.status == 200 else 'FAILED' }}"
          - "  Config Method: systemd drop-in (idempotent)"
          - ""
          - "üìÅ MODEL DIRECTORY CONTENTS:"
          - "{{ model_directory_contents.stdout_lines }}"
          - ""
          - "ü§ñ MODELS VIA API:"
          - "{{ final_model_check.json.models | length }} models detected"
          - ""
          - "üéØ ADDITIONAL MODELS:"
          - "  Status: {{ 'SUCCESS' if additional_model_pulls is succeeded else 'OPTIONAL - Not Available' }}"
          - "============================================================="

  handlers:
    - name: restart_ollama_fix
      systemd:
        name: ollama
        state: restarted
        daemon_reload: yes
