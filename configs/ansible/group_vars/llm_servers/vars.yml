---
# LLM Servers Configuration
# Group: llm_servers
# Servers: hx-llm-server-01, hx-llm-server-02

# Ollama Configuration
ollama:
  installer:
    url: "https://ollama.com/install.sh"
    checksum: "sha256:9f5f4c4ed21821ba9b847bf3607ae75452283276cd8f52d2f2b38ea9f27af344"
  service:
    host: "0.0.0.0"
    port: 11434
    origins: "*"
    environment:
      cuda_visible_devices: "0"
      cuda_path: "/usr/local/cuda-12.9"
  models:
    directory: "/opt/ai_models"

# LLM Models Configuration
chat_models:
  - name: "llama3.1"
    tag: "8b"
    size: "4.7GB"
  - name: "qwen2.5"
    tag: "7b"
    size: "4.4GB"
  - name: "mistral"
    tag: "7b"
    size: "4.1GB"

instruct_models:
  - name: "llama3.1"
    tag: "8b-instruct-q4_0"
    size: "4.7GB"
  - name: "qwen2.5"
    tag: "7b-instruct"
    size: "4.4GB"
  - name: "mistral"
    tag: "7b-instruct"
    size: "4.1GB"

# Performance Configuration
gpu_memory_fraction: 0.9
max_concurrent_requests: 4
model_timeout: 300

# Monitoring
monitoring_enabled: true
ollama_metrics_enabled: true
model_metrics_enabled: true
